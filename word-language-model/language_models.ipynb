{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b94a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "import model\n",
    "\n",
    "# Add ckp\n",
    "parser = argparse.ArgumentParser(description='PyTorch PennTreeBank RNN/LSTM Language Model')\n",
    "parser.add_argument('--data', type=str, default='/home/jamesjunior2/wikitext-2', # /input\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--checkpoint', type=str, default='',\n",
    "                    help='model checkpoint to use')\n",
    "parser.add_argument('--model', type=str, default='LSTM',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
    "parser.add_argument('--emsize', type=int, default=200,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhid', type=int, default=200,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--lr', type=float, default=20,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=40,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=35,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', action='store_true',\n",
    "                    help='tie the word embedding and softmax weights')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str,  default='/home/jamesjunior2/quantized_distillation/saved_MODELS/lang_model.pt', # /output\n",
    "                    help='path to save the final model')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d784dd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have a CUDA device, so you should probably run with --cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44108b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "corpus = data.Corpus(args.data)\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    if args.cuda:\n",
    "        data = data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20291bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, args.batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f9cf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   284, 15178,  ...,  1352,  1335,    16],\n",
       "        [    1,   357,    43,  ...,    46,    43,  2015],\n",
       "        [    2,  1496,  7369,  ...,   380,    27, 33001],\n",
       "        ...,\n",
       "        [  357,   415,   173,  ...,   212,    78,  1575],\n",
       "        [ 2520,     9,  3890,  ...,   208,    27,   808],\n",
       "        [   33,    35,    19,  ...,  8832,  6091,   209]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3843db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f07407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33278"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f48c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "if args.checkpoint != '':\n",
    "    if args.cuda:\n",
    "        model = torch.load(args.checkpoint)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        model = torch.load(args.checkpoint, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7128a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      "  (encoder): Embedding(33278, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
      "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()\n",
    "print (model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if args.cuda:\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7df440",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "        data, targets = get_batch(data_source, i, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f83cc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.data\n",
    "        #total_loss += loss_val.item()\n",
    "\n",
    "        if batch % args.log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt, lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3b3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesjunior2/anaconda3/envs/quantized_distenv/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/jamesjunior2/anaconda3/envs/quantized_distenv/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 141.06 | loss  7.62 | ppl  2038.69\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 142.99 | loss  6.85 | ppl   943.33\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 139.65 | loss  6.47 | ppl   647.76\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 145.64 | loss  6.29 | ppl   541.33\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 148.82 | loss  6.14 | ppl   465.67\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 146.42 | loss  6.06 | ppl   428.69\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 145.18 | loss  5.95 | ppl   382.33\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 148.35 | loss  5.96 | ppl   385.84\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 151.80 | loss  5.80 | ppl   330.54\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 151.06 | loss  5.78 | ppl   322.88\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 153.42 | loss  5.66 | ppl   287.40\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 146.73 | loss  5.67 | ppl   290.78\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 144.57 | loss  5.66 | ppl   285.95\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 154.77 | loss  5.54 | ppl   255.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesjunior2/anaconda3/envs/quantized_distenv/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 467.80s | valid loss  5.55 | valid ppl   257.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 153.09 | loss  5.55 | ppl   256.59\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 153.34 | loss  5.53 | ppl   253.40\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 149.13 | loss  5.36 | ppl   213.14\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 149.74 | loss  5.38 | ppl   216.79\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 149.24 | loss  5.36 | ppl   212.78\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 153.33 | loss  5.34 | ppl   207.84\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 157.24 | loss  5.33 | ppl   205.93\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 152.45 | loss  5.39 | ppl   219.56\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 157.94 | loss  5.26 | ppl   192.54\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 152.50 | loss  5.27 | ppl   195.09\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 146.59 | loss  5.18 | ppl   177.62\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 151.98 | loss  5.22 | ppl   184.34\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 154.88 | loss  5.22 | ppl   185.50\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 154.66 | loss  5.14 | ppl   170.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 482.97s | valid loss  5.30 | valid ppl   199.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 151.59 | loss  5.20 | ppl   180.56\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 149.24 | loss  5.21 | ppl   182.61\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 154.65 | loss  5.03 | ppl   152.42\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 150.73 | loss  5.07 | ppl   159.65\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 148.24 | loss  5.06 | ppl   158.00\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 150.12 | loss  5.06 | ppl   157.63\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 150.04 | loss  5.08 | ppl   161.37\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 153.47 | loss  5.15 | ppl   173.12\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 147.53 | loss  5.03 | ppl   153.00\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 144.87 | loss  5.06 | ppl   156.82\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 147.18 | loss  4.96 | ppl   142.02\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 154.24 | loss  5.00 | ppl   148.52\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 152.88 | loss  5.02 | ppl   151.32\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 152.27 | loss  4.93 | ppl   139.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 476.19s | valid loss  5.20 | valid ppl   180.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 147.38 | loss  5.00 | ppl   148.67\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 149.14 | loss  5.02 | ppl   151.99\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 153.10 | loss  4.84 | ppl   126.66\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 149.51 | loss  4.90 | ppl   134.20\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 148.50 | loss  4.90 | ppl   134.75\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 153.91 | loss  4.90 | ppl   134.43\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 153.52 | loss  4.94 | ppl   139.56\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 151.51 | loss  5.01 | ppl   149.39\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 147.96 | loss  4.89 | ppl   132.31\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 145.64 | loss  4.91 | ppl   135.51\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 147.04 | loss  4.81 | ppl   122.56\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 154.70 | loss  4.86 | ppl   128.90\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 150.80 | loss  4.88 | ppl   131.59\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 154.99 | loss  4.81 | ppl   122.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 476.05s | valid loss  5.10 | valid ppl   163.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 147.48 | loss  4.88 | ppl   131.20\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 149.10 | loss  4.90 | ppl   134.06\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 150.53 | loss  4.72 | ppl   112.11\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 151.41 | loss  4.78 | ppl   118.88\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 153.46 | loss  4.78 | ppl   119.41\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 156.51 | loss  4.79 | ppl   120.23\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 163.47 | loss  4.82 | ppl   124.39\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 148.35 | loss  4.90 | ppl   134.38\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 149.26 | loss  4.78 | ppl   119.07\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 151.47 | loss  4.81 | ppl   122.89\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 152.26 | loss  4.70 | ppl   110.47\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 156.58 | loss  4.76 | ppl   116.47\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 153.13 | loss  4.78 | ppl   119.48\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 153.84 | loss  4.71 | ppl   111.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 483.79s | valid loss  5.05 | valid ppl   156.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | lr 20.00 | ms/batch 152.58 | loss  4.78 | ppl   118.82\n",
      "| epoch   6 |   400/ 2983 batches | lr 20.00 | ms/batch 154.05 | loss  4.80 | ppl   121.62\n",
      "| epoch   6 |   600/ 2983 batches | lr 20.00 | ms/batch 155.80 | loss  4.62 | ppl   101.57\n",
      "| epoch   6 |   800/ 2983 batches | lr 20.00 | ms/batch 157.07 | loss  4.69 | ppl   108.86\n",
      "| epoch   6 |  1000/ 2983 batches | lr 20.00 | ms/batch 155.75 | loss  4.69 | ppl   109.24\n",
      "| epoch   6 |  1200/ 2983 batches | lr 20.00 | ms/batch 155.43 | loss  4.70 | ppl   110.43\n",
      "| epoch   6 |  1400/ 2983 batches | lr 20.00 | ms/batch 168.06 | loss  4.74 | ppl   114.77\n",
      "| epoch   6 |  1600/ 2983 batches | lr 20.00 | ms/batch 154.33 | loss  4.82 | ppl   124.01\n",
      "| epoch   6 |  1800/ 2983 batches | lr 20.00 | ms/batch 156.42 | loss  4.70 | ppl   110.14\n",
      "| epoch   6 |  2000/ 2983 batches | lr 20.00 | ms/batch 155.36 | loss  4.73 | ppl   113.68\n",
      "| epoch   6 |  2200/ 2983 batches | lr 20.00 | ms/batch 153.61 | loss  4.63 | ppl   102.38\n",
      "| epoch   6 |  2400/ 2983 batches | lr 20.00 | ms/batch 156.37 | loss  4.67 | ppl   106.95\n",
      "| epoch   6 |  2600/ 2983 batches | lr 20.00 | ms/batch 158.64 | loss  4.70 | ppl   110.42\n",
      "| epoch   6 |  2800/ 2983 batches | lr 20.00 | ms/batch 152.80 | loss  4.64 | ppl   103.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 491.92s | valid loss  5.01 | valid ppl   150.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2983 batches | lr 20.00 | ms/batch 157.26 | loss  4.70 | ppl   110.00\n",
      "| epoch   7 |   400/ 2983 batches | lr 20.00 | ms/batch 159.27 | loss  4.73 | ppl   113.11\n",
      "| epoch   7 |   600/ 2983 batches | lr 20.00 | ms/batch 157.07 | loss  4.55 | ppl    94.36\n",
      "| epoch   7 |   800/ 2983 batches | lr 20.00 | ms/batch 150.70 | loss  4.61 | ppl   100.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |  1000/ 2983 batches | lr 20.00 | ms/batch 156.81 | loss  4.63 | ppl   102.04\n",
      "| epoch   7 |  1200/ 2983 batches | lr 20.00 | ms/batch 158.00 | loss  4.63 | ppl   102.88\n",
      "| epoch   7 |  1400/ 2983 batches | lr 20.00 | ms/batch 149.20 | loss  4.67 | ppl   107.10\n",
      "| epoch   7 |  1600/ 2983 batches | lr 20.00 | ms/batch 155.46 | loss  4.75 | ppl   115.55\n",
      "| epoch   7 |  1800/ 2983 batches | lr 20.00 | ms/batch 152.71 | loss  4.64 | ppl   103.38\n",
      "| epoch   7 |  2000/ 2983 batches | lr 20.00 | ms/batch 153.38 | loss  4.67 | ppl   106.57\n",
      "| epoch   7 |  2200/ 2983 batches | lr 20.00 | ms/batch 147.32 | loss  4.57 | ppl    96.31\n",
      "| epoch   7 |  2400/ 2983 batches | lr 20.00 | ms/batch 154.46 | loss  4.61 | ppl   100.64\n",
      "| epoch   7 |  2600/ 2983 batches | lr 20.00 | ms/batch 154.69 | loss  4.64 | ppl   103.51\n",
      "| epoch   7 |  2800/ 2983 batches | lr 20.00 | ms/batch 151.12 | loss  4.57 | ppl    96.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 487.00s | valid loss  4.99 | valid ppl   146.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2983 batches | lr 20.00 | ms/batch 152.35 | loss  4.64 | ppl   103.65\n",
      "| epoch   8 |   400/ 2983 batches | lr 20.00 | ms/batch 154.45 | loss  4.67 | ppl   106.26\n",
      "| epoch   8 |   600/ 2983 batches | lr 20.00 | ms/batch 148.78 | loss  4.49 | ppl    89.04\n",
      "| epoch   8 |   800/ 2983 batches | lr 20.00 | ms/batch 146.78 | loss  4.55 | ppl    94.74\n",
      "| epoch   8 |  1000/ 2983 batches | lr 20.00 | ms/batch 147.80 | loss  4.57 | ppl    96.21\n",
      "| epoch   8 |  1200/ 2983 batches | lr 20.00 | ms/batch 152.64 | loss  4.58 | ppl    97.37\n",
      "| epoch   8 |  1400/ 2983 batches | lr 20.00 | ms/batch 154.96 | loss  4.61 | ppl   100.97\n",
      "| epoch   8 |  1600/ 2983 batches | lr 20.00 | ms/batch 155.36 | loss  4.70 | ppl   109.91\n",
      "| epoch   8 |  1800/ 2983 batches | lr 20.00 | ms/batch 155.11 | loss  4.59 | ppl    98.16\n",
      "| epoch   8 |  2000/ 2983 batches | lr 20.00 | ms/batch 152.95 | loss  4.62 | ppl   101.50\n",
      "| epoch   8 |  2200/ 2983 batches | lr 20.00 | ms/batch 149.67 | loss  4.51 | ppl    90.99\n",
      "| epoch   8 |  2400/ 2983 batches | lr 20.00 | ms/batch 152.45 | loss  4.56 | ppl    95.50\n",
      "| epoch   8 |  2600/ 2983 batches | lr 20.00 | ms/batch 146.72 | loss  4.59 | ppl    98.23\n",
      "| epoch   8 |  2800/ 2983 batches | lr 20.00 | ms/batch 149.21 | loss  4.53 | ppl    92.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 480.71s | valid loss  4.98 | valid ppl   146.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2983 batches | lr 20.00 | ms/batch 155.36 | loss  4.59 | ppl    98.08\n",
      "| epoch   9 |   400/ 2983 batches | lr 20.00 | ms/batch 159.67 | loss  4.61 | ppl   100.84\n",
      "| epoch   9 |   600/ 2983 batches | lr 20.00 | ms/batch 154.23 | loss  4.44 | ppl    84.85\n",
      "| epoch   9 |   800/ 2983 batches | lr 20.00 | ms/batch 150.27 | loss  4.50 | ppl    90.01\n",
      "| epoch   9 |  1000/ 2983 batches | lr 20.00 | ms/batch 153.09 | loss  4.52 | ppl    91.56\n",
      "| epoch   9 |  1200/ 2983 batches | lr 20.00 | ms/batch 155.21 | loss  4.53 | ppl    92.37\n",
      "| epoch   9 |  1400/ 2983 batches | lr 20.00 | ms/batch 151.92 | loss  4.57 | ppl    96.67\n",
      "| epoch   9 |  1600/ 2983 batches | lr 20.00 | ms/batch 149.43 | loss  4.65 | ppl   104.59\n",
      "| epoch   9 |  1800/ 2983 batches | lr 20.00 | ms/batch 159.22 | loss  4.55 | ppl    94.28\n",
      "| epoch   9 |  2000/ 2983 batches | lr 20.00 | ms/batch 155.25 | loss  4.58 | ppl    97.21\n",
      "| epoch   9 |  2200/ 2983 batches | lr 20.00 | ms/batch 149.54 | loss  4.47 | ppl    87.14\n",
      "| epoch   9 |  2400/ 2983 batches | lr 20.00 | ms/batch 153.53 | loss  4.51 | ppl    91.01\n",
      "| epoch   9 |  2600/ 2983 batches | lr 20.00 | ms/batch 152.37 | loss  4.54 | ppl    94.10\n",
      "| epoch   9 |  2800/ 2983 batches | lr 20.00 | ms/batch 153.69 | loss  4.49 | ppl    88.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 486.66s | valid loss  4.97 | valid ppl   144.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2983 batches | lr 20.00 | ms/batch 154.97 | loss  4.54 | ppl    94.15\n",
      "| epoch  10 |   400/ 2983 batches | lr 20.00 | ms/batch 151.21 | loss  4.57 | ppl    96.59\n",
      "| epoch  10 |   600/ 2983 batches | lr 20.00 | ms/batch 151.05 | loss  4.40 | ppl    81.65\n",
      "| epoch  10 |   800/ 2983 batches | lr 20.00 | ms/batch 155.34 | loss  4.46 | ppl    86.52\n",
      "| epoch  10 |  1000/ 2983 batches | lr 20.00 | ms/batch 152.21 | loss  4.48 | ppl    88.35\n",
      "| epoch  10 |  1200/ 2983 batches | lr 20.00 | ms/batch 150.86 | loss  4.48 | ppl    88.53\n",
      "| epoch  10 |  1400/ 2983 batches | lr 20.00 | ms/batch 151.59 | loss  4.53 | ppl    92.65\n",
      "| epoch  10 |  1600/ 2983 batches | lr 20.00 | ms/batch 155.20 | loss  4.60 | ppl    99.92\n",
      "| epoch  10 |  1800/ 2983 batches | lr 20.00 | ms/batch 148.00 | loss  4.51 | ppl    90.51\n",
      "| epoch  10 |  2000/ 2983 batches | lr 20.00 | ms/batch 149.42 | loss  4.54 | ppl    93.42\n",
      "| epoch  10 |  2200/ 2983 batches | lr 20.00 | ms/batch 153.65 | loss  4.43 | ppl    83.98\n",
      "| epoch  10 |  2400/ 2983 batches | lr 20.00 | ms/batch 156.94 | loss  4.48 | ppl    88.17\n",
      "| epoch  10 |  2600/ 2983 batches | lr 20.00 | ms/batch 152.67 | loss  4.51 | ppl    90.65\n",
      "| epoch  10 |  2800/ 2983 batches | lr 20.00 | ms/batch 151.25 | loss  4.45 | ppl    85.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 481.46s | valid loss  4.95 | valid ppl   140.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 2983 batches | lr 20.00 | ms/batch 152.59 | loss  4.51 | ppl    90.73\n",
      "| epoch  11 |   400/ 2983 batches | lr 20.00 | ms/batch 150.56 | loss  4.53 | ppl    92.70\n",
      "| epoch  11 |   600/ 2983 batches | lr 20.00 | ms/batch 148.36 | loss  4.36 | ppl    78.57\n",
      "| epoch  11 |   800/ 2983 batches | lr 20.00 | ms/batch 156.98 | loss  4.42 | ppl    82.85\n",
      "| epoch  11 |  1000/ 2983 batches | lr 20.00 | ms/batch 154.05 | loss  4.44 | ppl    85.11\n",
      "| epoch  11 |  1200/ 2983 batches | lr 20.00 | ms/batch 155.74 | loss  4.45 | ppl    85.90\n",
      "| epoch  11 |  1400/ 2983 batches | lr 20.00 | ms/batch 153.97 | loss  4.49 | ppl    89.42\n",
      "| epoch  11 |  1600/ 2983 batches | lr 20.00 | ms/batch 152.96 | loss  4.58 | ppl    97.07\n",
      "| epoch  11 |  1800/ 2983 batches | lr 20.00 | ms/batch 149.05 | loss  4.48 | ppl    87.84\n",
      "| epoch  11 |  2000/ 2983 batches | lr 20.00 | ms/batch 149.59 | loss  4.51 | ppl    90.93\n",
      "| epoch  11 |  2200/ 2983 batches | lr 20.00 | ms/batch 153.43 | loss  4.40 | ppl    81.62\n",
      "| epoch  11 |  2400/ 2983 batches | lr 20.00 | ms/batch 155.70 | loss  4.44 | ppl    85.04\n",
      "| epoch  11 |  2600/ 2983 batches | lr 20.00 | ms/batch 154.43 | loss  4.48 | ppl    87.90\n",
      "| epoch  11 |  2800/ 2983 batches | lr 20.00 | ms/batch 149.90 | loss  4.41 | ppl    82.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 482.16s | valid loss  4.96 | valid ppl   143.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 2983 batches | lr 5.00 | ms/batch 149.31 | loss  4.52 | ppl    91.79\n",
      "| epoch  12 |   400/ 2983 batches | lr 5.00 | ms/batch 150.99 | loss  4.50 | ppl    90.22\n",
      "| epoch  12 |   600/ 2983 batches | lr 5.00 | ms/batch 150.43 | loss  4.32 | ppl    75.39\n",
      "| epoch  12 |   800/ 2983 batches | lr 5.00 | ms/batch 148.35 | loss  4.37 | ppl    79.15\n",
      "| epoch  12 |  1000/ 2983 batches | lr 5.00 | ms/batch 157.98 | loss  4.37 | ppl    79.05\n",
      "| epoch  12 |  1200/ 2983 batches | lr 5.00 | ms/batch 155.03 | loss  4.37 | ppl    78.97\n",
      "| epoch  12 |  1400/ 2983 batches | lr 5.00 | ms/batch 152.93 | loss  4.38 | ppl    80.00\n",
      "| epoch  12 |  1600/ 2983 batches | lr 5.00 | ms/batch 151.49 | loss  4.44 | ppl    84.84\n",
      "| epoch  12 |  1800/ 2983 batches | lr 5.00 | ms/batch 154.57 | loss  4.33 | ppl    76.06\n",
      "| epoch  12 |  2000/ 2983 batches | lr 5.00 | ms/batch 157.07 | loss  4.36 | ppl    77.89\n",
      "| epoch  12 |  2200/ 2983 batches | lr 5.00 | ms/batch 156.09 | loss  4.22 | ppl    67.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 |  2400/ 2983 batches | lr 5.00 | ms/batch 156.09 | loss  4.25 | ppl    69.88\n",
      "| epoch  12 |  2600/ 2983 batches | lr 5.00 | ms/batch 152.27 | loss  4.26 | ppl    71.16\n",
      "| epoch  12 |  2800/ 2983 batches | lr 5.00 | ms/batch 153.65 | loss  4.19 | ppl    65.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 485.81s | valid loss  4.84 | valid ppl   126.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 2983 batches | lr 5.00 | ms/batch 155.64 | loss  4.38 | ppl    79.81\n",
      "| epoch  13 |   400/ 2983 batches | lr 5.00 | ms/batch 153.25 | loss  4.38 | ppl    80.23\n",
      "| epoch  13 |   600/ 2983 batches | lr 5.00 | ms/batch 153.97 | loss  4.22 | ppl    67.70\n",
      "| epoch  13 |   800/ 2983 batches | lr 5.00 | ms/batch 154.47 | loss  4.27 | ppl    71.53\n",
      "| epoch  13 |  1000/ 2983 batches | lr 5.00 | ms/batch 152.26 | loss  4.28 | ppl    72.50\n",
      "| epoch  13 |  1200/ 2983 batches | lr 5.00 | ms/batch 149.06 | loss  4.28 | ppl    72.56\n",
      "| epoch  13 |  1400/ 2983 batches | lr 5.00 | ms/batch 148.62 | loss  4.31 | ppl    74.65\n",
      "| epoch  13 |  1600/ 2983 batches | lr 5.00 | ms/batch 151.29 | loss  4.37 | ppl    79.24\n",
      "| epoch  13 |  1800/ 2983 batches | lr 5.00 | ms/batch 149.93 | loss  4.27 | ppl    71.18\n",
      "| epoch  13 |  2000/ 2983 batches | lr 5.00 | ms/batch 152.15 | loss  4.31 | ppl    74.12\n",
      "| epoch  13 |  2200/ 2983 batches | lr 5.00 | ms/batch 156.42 | loss  4.18 | ppl    65.19\n",
      "| epoch  13 |  2400/ 2983 batches | lr 5.00 | ms/batch 152.02 | loss  4.21 | ppl    67.28\n",
      "| epoch  13 |  2600/ 2983 batches | lr 5.00 | ms/batch 155.53 | loss  4.23 | ppl    68.86\n",
      "| epoch  13 |  2800/ 2983 batches | lr 5.00 | ms/batch 150.76 | loss  4.16 | ppl    64.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 483.61s | valid loss  4.83 | valid ppl   125.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 2983 batches | lr 5.00 | ms/batch 157.02 | loss  4.33 | ppl    75.70\n",
      "| epoch  14 |   400/ 2983 batches | lr 5.00 | ms/batch 152.90 | loss  4.34 | ppl    76.35\n",
      "| epoch  14 |   600/ 2983 batches | lr 5.00 | ms/batch 153.47 | loss  4.17 | ppl    64.39\n",
      "| epoch  14 |   800/ 2983 batches | lr 5.00 | ms/batch 149.77 | loss  4.22 | ppl    68.35\n",
      "| epoch  14 |  1000/ 2983 batches | lr 5.00 | ms/batch 151.78 | loss  4.24 | ppl    69.09\n",
      "| epoch  14 |  1200/ 2983 batches | lr 5.00 | ms/batch 150.45 | loss  4.25 | ppl    69.92\n",
      "| epoch  14 |  1400/ 2983 batches | lr 5.00 | ms/batch 157.49 | loss  4.27 | ppl    71.73\n",
      "| epoch  14 |  1600/ 2983 batches | lr 5.00 | ms/batch 152.40 | loss  4.34 | ppl    76.70\n",
      "| epoch  14 |  1800/ 2983 batches | lr 5.00 | ms/batch 155.84 | loss  4.24 | ppl    69.26\n",
      "| epoch  14 |  2000/ 2983 batches | lr 5.00 | ms/batch 150.29 | loss  4.27 | ppl    71.80\n",
      "| epoch  14 |  2200/ 2983 batches | lr 5.00 | ms/batch 151.50 | loss  4.15 | ppl    63.29\n",
      "| epoch  14 |  2400/ 2983 batches | lr 5.00 | ms/batch 159.56 | loss  4.18 | ppl    65.66\n",
      "| epoch  14 |  2600/ 2983 batches | lr 5.00 | ms/batch 159.80 | loss  4.21 | ppl    67.29\n",
      "| epoch  14 |  2800/ 2983 batches | lr 5.00 | ms/batch 153.87 | loss  4.14 | ppl    63.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 487.39s | valid loss  4.83 | valid ppl   124.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 2983 batches | lr 5.00 | ms/batch 146.89 | loss  4.29 | ppl    73.18\n",
      "| epoch  15 |   400/ 2983 batches | lr 5.00 | ms/batch 149.93 | loss  4.30 | ppl    73.79\n",
      "| epoch  15 |   600/ 2983 batches | lr 5.00 | ms/batch 151.15 | loss  4.13 | ppl    62.23\n",
      "| epoch  15 |   800/ 2983 batches | lr 5.00 | ms/batch 151.75 | loss  4.19 | ppl    65.99\n",
      "| epoch  15 |  1000/ 2983 batches | lr 5.00 | ms/batch 155.65 | loss  4.21 | ppl    67.35\n",
      "| epoch  15 |  1200/ 2983 batches | lr 5.00 | ms/batch 152.28 | loss  4.21 | ppl    67.69\n",
      "| epoch  15 |  1400/ 2983 batches | lr 5.00 | ms/batch 148.36 | loss  4.25 | ppl    69.94\n",
      "| epoch  15 |  1600/ 2983 batches | lr 5.00 | ms/batch 150.53 | loss  4.31 | ppl    74.40\n",
      "| epoch  15 |  1800/ 2983 batches | lr 5.00 | ms/batch 151.00 | loss  4.21 | ppl    67.26\n",
      "| epoch  15 |  2000/ 2983 batches | lr 5.00 | ms/batch 159.05 | loss  4.26 | ppl    70.75\n",
      "| epoch  15 |  2200/ 2983 batches | lr 5.00 | ms/batch 153.90 | loss  4.13 | ppl    62.00\n",
      "| epoch  15 |  2400/ 2983 batches | lr 5.00 | ms/batch 153.53 | loss  4.16 | ppl    64.30\n",
      "| epoch  15 |  2600/ 2983 batches | lr 5.00 | ms/batch 152.53 | loss  4.19 | ppl    65.91\n",
      "| epoch  15 |  2800/ 2983 batches | lr 5.00 | ms/batch 155.81 | loss  4.13 | ppl    61.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 482.59s | valid loss  4.82 | valid ppl   123.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 2983 batches | lr 5.00 | ms/batch 150.70 | loss  4.27 | ppl    71.17\n",
      "| epoch  16 |   400/ 2983 batches | lr 5.00 | ms/batch 153.11 | loss  4.28 | ppl    71.94\n",
      "| epoch  16 |   600/ 2983 batches | lr 5.00 | ms/batch 146.21 | loss  4.11 | ppl    60.69\n",
      "| epoch  16 |   800/ 2983 batches | lr 5.00 | ms/batch 154.40 | loss  4.16 | ppl    64.14\n",
      "| epoch  16 |  1000/ 2983 batches | lr 5.00 | ms/batch 150.68 | loss  4.19 | ppl    65.73\n",
      "| epoch  16 |  1200/ 2983 batches | lr 5.00 | ms/batch 151.84 | loss  4.20 | ppl    66.52\n",
      "| epoch  16 |  1400/ 2983 batches | lr 5.00 | ms/batch 154.77 | loss  4.23 | ppl    68.88\n",
      "| epoch  16 |  1600/ 2983 batches | lr 5.00 | ms/batch 151.52 | loss  4.29 | ppl    72.98\n",
      "| epoch  16 |  1800/ 2983 batches | lr 5.00 | ms/batch 151.42 | loss  4.19 | ppl    66.28\n",
      "| epoch  16 |  2000/ 2983 batches | lr 5.00 | ms/batch 153.81 | loss  4.23 | ppl    68.88\n",
      "| epoch  16 |  2200/ 2983 batches | lr 5.00 | ms/batch 146.64 | loss  4.11 | ppl    61.09\n",
      "| epoch  16 |  2400/ 2983 batches | lr 5.00 | ms/batch 155.47 | loss  4.14 | ppl    63.01\n",
      "| epoch  16 |  2600/ 2983 batches | lr 5.00 | ms/batch 158.23 | loss  4.18 | ppl    65.31\n",
      "| epoch  16 |  2800/ 2983 batches | lr 5.00 | ms/batch 153.04 | loss  4.12 | ppl    61.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 483.19s | valid loss  4.82 | valid ppl   124.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 2983 batches | lr 1.25 | ms/batch 153.59 | loss  4.29 | ppl    73.19\n",
      "| epoch  17 |   400/ 2983 batches | lr 1.25 | ms/batch 152.35 | loss  4.31 | ppl    74.44\n",
      "| epoch  17 |   600/ 2983 batches | lr 1.25 | ms/batch 150.11 | loss  4.12 | ppl    61.54\n",
      "| epoch  17 |   800/ 2983 batches | lr 1.25 | ms/batch 150.16 | loss  4.20 | ppl    66.39\n",
      "| epoch  17 |  1000/ 2983 batches | lr 1.25 | ms/batch 150.79 | loss  4.20 | ppl    66.43\n",
      "| epoch  17 |  1200/ 2983 batches | lr 1.25 | ms/batch 148.99 | loss  4.20 | ppl    66.71\n",
      "| epoch  17 |  1400/ 2983 batches | lr 1.25 | ms/batch 147.97 | loss  4.23 | ppl    68.40\n",
      "| epoch  17 |  1600/ 2983 batches | lr 1.25 | ms/batch 155.37 | loss  4.28 | ppl    72.15\n",
      "| epoch  17 |  1800/ 2983 batches | lr 1.25 | ms/batch 149.67 | loss  4.18 | ppl    65.07\n",
      "| epoch  17 |  2000/ 2983 batches | lr 1.25 | ms/batch 152.12 | loss  4.21 | ppl    67.05\n",
      "| epoch  17 |  2200/ 2983 batches | lr 1.25 | ms/batch 156.56 | loss  4.07 | ppl    58.54\n",
      "| epoch  17 |  2400/ 2983 batches | lr 1.25 | ms/batch 151.81 | loss  4.11 | ppl    60.83\n",
      "| epoch  17 |  2600/ 2983 batches | lr 1.25 | ms/batch 151.80 | loss  4.14 | ppl    62.49\n",
      "| epoch  17 |  2800/ 2983 batches | lr 1.25 | ms/batch 151.92 | loss  4.05 | ppl    57.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 478.52s | valid loss  4.78 | valid ppl   119.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 2983 batches | lr 1.25 | ms/batch 153.96 | loss  4.26 | ppl    70.52\n",
      "| epoch  18 |   400/ 2983 batches | lr 1.25 | ms/batch 152.06 | loss  4.27 | ppl    71.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  18 |   600/ 2983 batches | lr 1.25 | ms/batch 151.81 | loss  4.09 | ppl    59.65\n",
      "| epoch  18 |   800/ 2983 batches | lr 1.25 | ms/batch 147.24 | loss  4.16 | ppl    64.21\n",
      "| epoch  18 |  1000/ 2983 batches | lr 1.25 | ms/batch 184.38 | loss  4.16 | ppl    64.32\n",
      "| epoch  18 |  1200/ 2983 batches | lr 1.25 | ms/batch 155.65 | loss  4.18 | ppl    65.15\n",
      "| epoch  18 |  1400/ 2983 batches | lr 1.25 | ms/batch 152.81 | loss  4.20 | ppl    66.59\n",
      "| epoch  18 |  1600/ 2983 batches | lr 1.25 | ms/batch 150.10 | loss  4.25 | ppl    70.37\n",
      "| epoch  18 |  1800/ 2983 batches | lr 1.25 | ms/batch 152.86 | loss  4.16 | ppl    63.83\n",
      "| epoch  18 |  2000/ 2983 batches | lr 1.25 | ms/batch 149.66 | loss  4.20 | ppl    66.44\n",
      "| epoch  18 |  2200/ 2983 batches | lr 1.25 | ms/batch 161.96 | loss  4.06 | ppl    58.15\n",
      "| epoch  18 |  2400/ 2983 batches | lr 1.25 | ms/batch 148.27 | loss  4.10 | ppl    60.25\n",
      "| epoch  18 |  2600/ 2983 batches | lr 1.25 | ms/batch 148.99 | loss  4.13 | ppl    61.89\n",
      "| epoch  18 |  2800/ 2983 batches | lr 1.25 | ms/batch 154.57 | loss  4.06 | ppl    57.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 487.43s | valid loss  4.78 | valid ppl   118.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 2983 batches | lr 1.25 | ms/batch 147.71 | loss  4.24 | ppl    69.33\n",
      "| epoch  19 |   400/ 2983 batches | lr 1.25 | ms/batch 151.55 | loss  4.25 | ppl    70.23\n",
      "| epoch  19 |   600/ 2983 batches | lr 1.25 | ms/batch 153.22 | loss  4.08 | ppl    58.85\n",
      "| epoch  19 |   800/ 2983 batches | lr 1.25 | ms/batch 148.58 | loss  4.15 | ppl    63.29\n",
      "| epoch  19 |  1000/ 2983 batches | lr 1.25 | ms/batch 154.96 | loss  4.15 | ppl    63.47\n",
      "| epoch  19 |  1200/ 2983 batches | lr 1.25 | ms/batch 154.68 | loss  4.17 | ppl    64.59\n",
      "| epoch  19 |  1400/ 2983 batches | lr 1.25 | ms/batch 151.60 | loss  4.19 | ppl    65.82\n",
      "| epoch  19 |  1600/ 2983 batches | lr 1.25 | ms/batch 153.05 | loss  4.24 | ppl    69.67\n",
      "| epoch  19 |  1800/ 2983 batches | lr 1.25 | ms/batch 151.99 | loss  4.15 | ppl    63.20\n",
      "| epoch  19 |  2000/ 2983 batches | lr 1.25 | ms/batch 158.55 | loss  4.19 | ppl    65.76\n",
      "| epoch  19 |  2200/ 2983 batches | lr 1.25 | ms/batch 153.02 | loss  4.05 | ppl    57.56\n",
      "| epoch  19 |  2400/ 2983 batches | lr 1.25 | ms/batch 156.63 | loss  4.09 | ppl    59.67\n",
      "| epoch  19 |  2600/ 2983 batches | lr 1.25 | ms/batch 149.53 | loss  4.12 | ppl    61.77\n",
      "| epoch  19 |  2800/ 2983 batches | lr 1.25 | ms/batch 153.06 | loss  4.05 | ppl    57.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 483.93s | valid loss  4.78 | valid ppl   118.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 2983 batches | lr 1.25 | ms/batch 155.10 | loss  4.23 | ppl    68.60\n",
      "| epoch  20 |   400/ 2983 batches | lr 1.25 | ms/batch 152.27 | loss  4.24 | ppl    69.67\n",
      "| epoch  20 |   600/ 2983 batches | lr 1.25 | ms/batch 153.06 | loss  4.06 | ppl    58.02\n",
      "| epoch  20 |   800/ 2983 batches | lr 1.25 | ms/batch 154.17 | loss  4.14 | ppl    62.57\n",
      "| epoch  20 |  1000/ 2983 batches | lr 1.25 | ms/batch 156.89 | loss  4.14 | ppl    62.54\n",
      "| epoch  20 |  1200/ 2983 batches | lr 1.25 | ms/batch 150.22 | loss  4.15 | ppl    63.67\n",
      "| epoch  20 |  1400/ 2983 batches | lr 1.25 | ms/batch 150.52 | loss  4.18 | ppl    65.45\n",
      "| epoch  20 |  1600/ 2983 batches | lr 1.25 | ms/batch 164.42 | loss  4.24 | ppl    69.11\n",
      "| epoch  20 |  1800/ 2983 batches | lr 1.25 | ms/batch 152.26 | loss  4.14 | ppl    62.82\n",
      "| epoch  20 |  2000/ 2983 batches | lr 1.25 | ms/batch 148.73 | loss  4.18 | ppl    65.30\n",
      "| epoch  20 |  2200/ 2983 batches | lr 1.25 | ms/batch 147.41 | loss  4.05 | ppl    57.26\n",
      "| epoch  20 |  2400/ 2983 batches | lr 1.25 | ms/batch 153.52 | loss  4.09 | ppl    59.62\n",
      "| epoch  20 |  2600/ 2983 batches | lr 1.25 | ms/batch 148.34 | loss  4.12 | ppl    61.55\n",
      "| epoch  20 |  2800/ 2983 batches | lr 1.25 | ms/batch 152.62 | loss  4.05 | ppl    57.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 481.72s | valid loss  4.78 | valid ppl   118.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/ 2983 batches | lr 1.25 | ms/batch 149.71 | loss  4.22 | ppl    67.82\n",
      "| epoch  21 |   400/ 2983 batches | lr 1.25 | ms/batch 150.98 | loss  4.23 | ppl    68.63\n",
      "| epoch  21 |   600/ 2983 batches | lr 1.25 | ms/batch 153.28 | loss  4.05 | ppl    57.47\n",
      "| epoch  21 |   800/ 2983 batches | lr 1.25 | ms/batch 152.94 | loss  4.13 | ppl    62.02\n",
      "| epoch  21 |  1000/ 2983 batches | lr 1.25 | ms/batch 148.38 | loss  4.13 | ppl    62.27\n",
      "| epoch  21 |  1200/ 2983 batches | lr 1.25 | ms/batch 153.51 | loss  4.15 | ppl    63.19\n",
      "| epoch  21 |  1400/ 2983 batches | lr 1.25 | ms/batch 146.96 | loss  4.17 | ppl    64.96\n",
      "| epoch  21 |  1600/ 2983 batches | lr 1.25 | ms/batch 150.39 | loss  4.23 | ppl    68.47\n",
      "| epoch  21 |  1800/ 2983 batches | lr 1.25 | ms/batch 155.27 | loss  4.13 | ppl    62.45\n",
      "| epoch  21 |  2000/ 2983 batches | lr 1.25 | ms/batch 150.74 | loss  4.17 | ppl    64.98\n",
      "| epoch  21 |  2200/ 2983 batches | lr 1.25 | ms/batch 152.72 | loss  4.04 | ppl    56.80\n",
      "| epoch  21 |  2400/ 2983 batches | lr 1.25 | ms/batch 148.98 | loss  4.08 | ppl    59.24\n",
      "| epoch  21 |  2600/ 2983 batches | lr 1.25 | ms/batch 158.57 | loss  4.12 | ppl    61.41\n",
      "| epoch  21 |  2800/ 2983 batches | lr 1.25 | ms/batch 151.47 | loss  4.04 | ppl    56.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 479.41s | valid loss  4.78 | valid ppl   118.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/ 2983 batches | lr 0.31 | ms/batch 152.08 | loss  4.23 | ppl    68.46\n",
      "| epoch  22 |   400/ 2983 batches | lr 0.31 | ms/batch 152.00 | loss  4.27 | ppl    71.57\n",
      "| epoch  22 |   600/ 2983 batches | lr 0.31 | ms/batch 151.66 | loss  4.08 | ppl    59.23\n",
      "| epoch  22 |   800/ 2983 batches | lr 0.31 | ms/batch 147.86 | loss  4.16 | ppl    63.84\n",
      "| epoch  22 |  1000/ 2983 batches | lr 0.31 | ms/batch 148.53 | loss  4.15 | ppl    63.41\n",
      "| epoch  22 |  1200/ 2983 batches | lr 0.31 | ms/batch 147.72 | loss  4.15 | ppl    63.38\n",
      "| epoch  22 |  1400/ 2983 batches | lr 0.31 | ms/batch 151.58 | loss  4.18 | ppl    65.18\n",
      "| epoch  22 |  1600/ 2983 batches | lr 0.31 | ms/batch 153.20 | loss  4.23 | ppl    68.90\n",
      "| epoch  22 |  1800/ 2983 batches | lr 0.31 | ms/batch 150.39 | loss  4.14 | ppl    62.57\n",
      "| epoch  22 |  2000/ 2983 batches | lr 0.31 | ms/batch 156.25 | loss  4.18 | ppl    65.16\n",
      "| epoch  22 |  2200/ 2983 batches | lr 0.31 | ms/batch 151.11 | loss  4.04 | ppl    56.71\n",
      "| epoch  22 |  2400/ 2983 batches | lr 0.31 | ms/batch 148.21 | loss  4.07 | ppl    58.34\n",
      "| epoch  22 |  2600/ 2983 batches | lr 0.31 | ms/batch 154.26 | loss  4.11 | ppl    60.99\n",
      "| epoch  22 |  2800/ 2983 batches | lr 0.31 | ms/batch 151.11 | loss  4.03 | ppl    56.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 477.99s | valid loss  4.76 | valid ppl   116.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/ 2983 batches | lr 0.31 | ms/batch 151.17 | loss  4.21 | ppl    67.61\n",
      "| epoch  23 |   400/ 2983 batches | lr 0.31 | ms/batch 157.07 | loss  4.25 | ppl    69.94\n",
      "| epoch  23 |   600/ 2983 batches | lr 0.31 | ms/batch 148.48 | loss  4.06 | ppl    58.17\n",
      "| epoch  23 |   800/ 2983 batches | lr 0.31 | ms/batch 154.75 | loss  4.14 | ppl    63.03\n",
      "| epoch  23 |  1000/ 2983 batches | lr 0.31 | ms/batch 154.72 | loss  4.14 | ppl    62.52\n",
      "| epoch  23 |  1200/ 2983 batches | lr 0.31 | ms/batch 152.85 | loss  4.15 | ppl    63.31\n",
      "| epoch  23 |  1400/ 2983 batches | lr 0.31 | ms/batch 152.77 | loss  4.17 | ppl    64.91\n",
      "| epoch  23 |  1600/ 2983 batches | lr 0.31 | ms/batch 155.01 | loss  4.23 | ppl    68.45\n",
      "| epoch  23 |  1800/ 2983 batches | lr 0.31 | ms/batch 151.80 | loss  4.13 | ppl    62.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  23 |  2000/ 2983 batches | lr 0.31 | ms/batch 150.37 | loss  4.17 | ppl    64.93\n",
      "| epoch  23 |  2200/ 2983 batches | lr 0.31 | ms/batch 146.41 | loss  4.04 | ppl    56.66\n",
      "| epoch  23 |  2400/ 2983 batches | lr 0.31 | ms/batch 151.59 | loss  4.07 | ppl    58.32\n",
      "| epoch  23 |  2600/ 2983 batches | lr 0.31 | ms/batch 151.93 | loss  4.11 | ppl    60.80\n",
      "| epoch  23 |  2800/ 2983 batches | lr 0.31 | ms/batch 156.39 | loss  4.03 | ppl    56.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 481.70s | valid loss  4.76 | valid ppl   116.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/ 2983 batches | lr 0.31 | ms/batch 150.30 | loss  4.21 | ppl    67.26\n",
      "| epoch  24 |   400/ 2983 batches | lr 0.31 | ms/batch 150.67 | loss  4.24 | ppl    69.45\n",
      "| epoch  24 |   600/ 2983 batches | lr 0.31 | ms/batch 154.50 | loss  4.06 | ppl    57.69\n",
      "| epoch  24 |   800/ 2983 batches | lr 0.31 | ms/batch 151.62 | loss  4.14 | ppl    62.70\n",
      "| epoch  24 |  1000/ 2983 batches | lr 0.31 | ms/batch 155.84 | loss  4.13 | ppl    62.25\n",
      "| epoch  24 |  1200/ 2983 batches | lr 0.31 | ms/batch 149.89 | loss  4.14 | ppl    62.95\n",
      "| epoch  24 |  1400/ 2983 batches | lr 0.31 | ms/batch 150.54 | loss  4.17 | ppl    64.85\n",
      "| epoch  24 |  1600/ 2983 batches | lr 0.31 | ms/batch 154.01 | loss  4.23 | ppl    68.40\n",
      "| epoch  24 |  1800/ 2983 batches | lr 0.31 | ms/batch 153.41 | loss  4.13 | ppl    62.06\n",
      "| epoch  24 |  2000/ 2983 batches | lr 0.31 | ms/batch 155.98 | loss  4.17 | ppl    64.69\n",
      "| epoch  24 |  2200/ 2983 batches | lr 0.31 | ms/batch 151.41 | loss  4.04 | ppl    56.66\n",
      "| epoch  24 |  2400/ 2983 batches | lr 0.31 | ms/batch 149.88 | loss  4.07 | ppl    58.62\n",
      "| epoch  24 |  2600/ 2983 batches | lr 0.31 | ms/batch 153.77 | loss  4.11 | ppl    60.85\n",
      "| epoch  24 |  2800/ 2983 batches | lr 0.31 | ms/batch 148.03 | loss  4.03 | ppl    56.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 480.46s | valid loss  4.76 | valid ppl   116.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/ 2983 batches | lr 0.31 | ms/batch 144.82 | loss  4.21 | ppl    67.24\n",
      "| epoch  25 |   400/ 2983 batches | lr 0.31 | ms/batch 152.75 | loss  4.23 | ppl    69.06\n",
      "| epoch  25 |   600/ 2983 batches | lr 0.31 | ms/batch 153.37 | loss  4.06 | ppl    57.94\n",
      "| epoch  25 |   800/ 2983 batches | lr 0.31 | ms/batch 150.53 | loss  4.13 | ppl    62.44\n",
      "| epoch  25 |  1000/ 2983 batches | lr 0.31 | ms/batch 151.76 | loss  4.13 | ppl    61.99\n",
      "| epoch  25 |  1200/ 2983 batches | lr 0.31 | ms/batch 150.34 | loss  4.14 | ppl    62.64\n",
      "| epoch  25 |  1400/ 2983 batches | lr 0.31 | ms/batch 154.96 | loss  4.17 | ppl    64.42\n",
      "| epoch  25 |  1600/ 2983 batches | lr 0.31 | ms/batch 147.68 | loss  4.23 | ppl    68.43\n",
      "| epoch  25 |  1800/ 2983 batches | lr 0.31 | ms/batch 144.77 | loss  4.13 | ppl    61.98\n",
      "| epoch  25 |  2000/ 2983 batches | lr 0.31 | ms/batch 150.48 | loss  4.17 | ppl    64.53\n",
      "| epoch  25 |  2200/ 2983 batches | lr 0.31 | ms/batch 156.42 | loss  4.04 | ppl    56.74\n",
      "| epoch  25 |  2400/ 2983 batches | lr 0.31 | ms/batch 155.95 | loss  4.07 | ppl    58.61\n",
      "| epoch  25 |  2600/ 2983 batches | lr 0.31 | ms/batch 150.88 | loss  4.11 | ppl    60.68\n",
      "| epoch  25 |  2800/ 2983 batches | lr 0.31 | ms/batch 155.27 | loss  4.03 | ppl    56.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 480.60s | valid loss  4.75 | valid ppl   116.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/ 2983 batches | lr 0.31 | ms/batch 149.11 | loss  4.21 | ppl    67.17\n",
      "| epoch  26 |   400/ 2983 batches | lr 0.31 | ms/batch 152.38 | loss  4.23 | ppl    68.83\n",
      "| epoch  26 |   600/ 2983 batches | lr 0.31 | ms/batch 153.68 | loss  4.05 | ppl    57.51\n",
      "| epoch  26 |   800/ 2983 batches | lr 0.31 | ms/batch 154.07 | loss  4.13 | ppl    62.33\n",
      "| epoch  26 |  1000/ 2983 batches | lr 0.31 | ms/batch 149.75 | loss  4.13 | ppl    62.03\n",
      "| epoch  26 |  1200/ 2983 batches | lr 0.31 | ms/batch 152.49 | loss  4.14 | ppl    62.72\n",
      "| epoch  26 |  1400/ 2983 batches | lr 0.31 | ms/batch 151.88 | loss  4.16 | ppl    64.39\n",
      "| epoch  26 |  1600/ 2983 batches | lr 0.31 | ms/batch 150.02 | loss  4.22 | ppl    68.00\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "lr = args.lr\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93d6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
