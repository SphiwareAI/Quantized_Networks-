# Quantized_Networks-

In this work we use quantization and distillation to compress deep learning models. We show that quantization and distillation is a better compression model compared to low-rank compression models like Tensor Networks. 
